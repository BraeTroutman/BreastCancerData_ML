{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Diagnostic with Adaline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd # for loading data csv into dataframe and cleaning data\n",
    "import os # for building url path\n",
    "import numpy as np \n",
    "import matplotlib.colors # will use ListedColorMap to plot results\n",
    "import matplotlib.pyplot as pp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read cancer data from ML database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>thickness</th>\n",
       "      <th>size uniformity</th>\n",
       "      <th>shape uniformity</th>\n",
       "      <th>marginal adhesion</th>\n",
       "      <th>epi cell size</th>\n",
       "      <th>bare nuclei</th>\n",
       "      <th>bland chromatin</th>\n",
       "      <th>normal nucleoi</th>\n",
       "      <th>mitoses</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1000025</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1002945</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1015425</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1016277</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1017023</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id  thickness  size uniformity  shape uniformity  marginal adhesion  \\\n",
       "0  1000025          5                1                 1                  1   \n",
       "1  1002945          5                4                 4                  5   \n",
       "2  1015425          3                1                 1                  1   \n",
       "3  1016277          6                8                 8                  1   \n",
       "4  1017023          4                1                 1                  3   \n",
       "\n",
       "   epi cell size bare nuclei  bland chromatin  normal nucleoi  mitoses  class  \n",
       "0              2           1                3               1        1      2  \n",
       "1              7          10                3               2        1      2  \n",
       "2              2           2                3               1        1      2  \n",
       "3              3           4                3               7        1      2  \n",
       "4              2           1                3               1        1      2  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer_data_url = os.path.join ('https://archive.ics.uci.edu', 'ml', 'machine-learning-databases', 'breast-cancer-wisconsin', 'breast-cancer-wisconsin.data')\n",
    "cancer_data = pd.read_csv(cancer_data_url, header=None, encoding='utf-8')\n",
    "cancer_data.columns = ['id', 'thickness', 'size uniformity', # assign columns more useful names\n",
    "                       'shape uniformity', 'marginal adhesion',\n",
    "                       'epi cell size', 'bare nuclei', 'bland chromatin',\n",
    "                       'normal nucleoi', 'mitoses', 'class']\n",
    "cancer_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Fn to return a list of every unique grouping of three values in a given list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def every_unique_pair(mylist):\n",
    "    \"\"\"returns every positionally unique pair in a given list: assumes that no elements in the list appear more than once\"\"\"\n",
    "    ret_list = []\n",
    "    list_copy = mylist[1:].copy()\n",
    "    for i in mylist:\n",
    "        for j in list_copy:\n",
    "            ret_list.append([i,j])\n",
    "        list_copy = list_copy[1:]\n",
    "    return ret_list\n",
    "\n",
    "# figure,subplotx = pp.subplots(len(every_unique_pair(cancer_data.columns[1:len(cancer_data.columns)-1])))\n",
    "# figure.suptitle('Visual Comparison of Linear Separability')\n",
    "# figure.set_size_inches(20, 40)\n",
    "# for i, pair in enumerate(every_unique_pair(cancer_data.columns[1:len(cancer_data.columns) - 1])):\n",
    "#     c1,c2 = pair\n",
    "#     plot_classes_by_columns(subplotx[i], cancer_data, c1, c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adaline SGD Classifier Class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### As implemented in Ch02"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdalineSGD(object):\n",
    "    \"\"\"ADAptive LInear NEuron classifier.\"\"\"\n",
    "    \n",
    "    # I added the keyword parameter threshold to allow the user to specify the threshold\n",
    "    def __init__(self, learning_rate=0.01, epochs=10, shuffle=True, random_seed=None, threshold=0):\n",
    "        \"\"\"Initialize the Adaline object\"\"\"\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        self.weights_initialized = False\n",
    "        self.shuffle = shuffle\n",
    "        self.random_seed = random_seed\n",
    "        self.threshold = threshold\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Initialize and iteratively update weights\"\"\"\n",
    "        self._initialize_weights(X.shape[1])\n",
    "        self.cost_ = []\n",
    "        for i in range(self.epochs):\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "            cost = []\n",
    "            for xi, target in zip(X, y):\n",
    "                cost.append(self._update_weights(xi, target))\n",
    "            avg_cost = sum(cost) / len(y)\n",
    "            self.cost_.append(avg_cost)\n",
    "        return self\n",
    "\n",
    "    def partial_fit(self, X, y):\n",
    "        \"\"\"Fit training data without reinitializing the weights\"\"\"\n",
    "        if not self.weights_initialized:\n",
    "            self._initialize_weights(X.shape[1])\n",
    "        if y.ravel().shape[0] > 1:\n",
    "            for xi, target in zip(X, y):\n",
    "                self._update_weights(xi, target)\n",
    "        else:\n",
    "            self._update_weights(X, y)\n",
    "        return self\n",
    "\n",
    "    def _shuffle(self, X, y):\n",
    "        \"\"\"Shuffle training data\"\"\"\n",
    "        r = self.rgen.permutation(len(y))\n",
    "        return X[r], y[r]\n",
    "    \n",
    "    def _initialize_weights(self, m):\n",
    "        \"\"\"Initialize weights to small random numbers\"\"\"\n",
    "        self.rgen = np.random.RandomState(self.random_seed)\n",
    "        self.weights = self.rgen.normal(loc=0.0, scale=0.01, size=1 + m)\n",
    "        self.weights_initialized = True\n",
    "        \n",
    "    def _update_weights(self, xi, target):\n",
    "        \"\"\"Apply Adaline learning rule to update the weights\"\"\"\n",
    "        output = self.activation(self.net_input(xi))\n",
    "        error = (target - output)\n",
    "        self.weights[1:] += self.learning_rate * xi.dot(error)\n",
    "        self.weights[0] += self.learning_rate * error\n",
    "        cost = 0.5 * error**2\n",
    "        return cost\n",
    "    \n",
    "    def net_input(self, X):\n",
    "        \"\"\"Calculate net input\"\"\"\n",
    "        return np.dot(X, self.weights[1:]) + self.weights[0]\n",
    "\n",
    "    def activation(self, X):\n",
    "        \"\"\"Compute linear activation\"\"\"\n",
    "        return X\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        return np.where(self.activation(self.net_input(X)) >= self.threshold, 1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fns for running Adaline and analyzing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_and_misclasses(prediction, labels):\n",
    "    \"\"\"Fn to determine accuracy\"\"\"\n",
    "    missclassifications = 0\n",
    "    correct_predictions = len(labels)\n",
    "    for a,b in zip(prediction, labels):\n",
    "        if a != b:\n",
    "            missclassifications += 1\n",
    "            correct_predictions -= 1\n",
    "    return (correct_predictions / len(labels), missclassifications)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_fit_test(c1, c2, c3, testtrain_ratio, dataframe=cancer_data, verbose=False, learning_rate=0.1, epochs=50, threshold=0):\n",
    "    \"\"\"split data from feature columns c1 and c2 into train and test sets at tt_ratio proportions and fit/test a perceptron\"\"\"\n",
    "    \n",
    "    # get the integer indeces corresponding to the column names passed to split_fit_test\n",
    "    c1_idx = dataframe.columns.get_loc(c1)\n",
    "    c2_idx = dataframe.columns.get_loc(c2)\n",
    "    c3_idx = dataframe.columns.get_loc(c2)\n",
    "    \n",
    "    # number of rows of dataframe which will belong to the training set (we know the number in the test set from this implicitly)\n",
    "    num_train = len(dataframe.index) - int(len(dataframe.index) * testtrain_ratio)\n",
    "    \n",
    "    # Training set\n",
    "    y_train = cancer_data.iloc[:num_train,10].values # the array of target values: 2 for benign, 4 for malignant\n",
    "    y_train = np.where(y_train == 2, -1, 1) # change class labels 2 and 4 to -1 and 1 respectively\n",
    "    X_train = cancer_data.iloc[:num_train, [c1_idx,c2_idx,c3_idx]].values\n",
    "    \n",
    "    # feature scaling to standardize the distribution of values in our training set\n",
    "    X_train_std = np.copy(X_train)\n",
    "    X_train_std[:, 0] = (X_train[:, 0] - X_train[:, 0].mean()) / X_train[:, 0].std()\n",
    "    X_train_std[:, 1] = (X_train[:, 1] - X_train[:, 1].mean()) / X_train[:, 1].std()\n",
    "    X_train_std[:, 2] = (X_train[:, 2] - X_train[:, 2].mean()) / X_train[:, 2].std()\n",
    "    \n",
    "    # Testing set\n",
    "    y_test = cancer_data.iloc[num_train:,10].values # analagous to above\n",
    "    y_test = np.where(y_test == 2, -1, 1)\n",
    "    X_test = cancer_data.iloc[num_train:, [c1_idx, c2_idx, c3_idx]].values\n",
    "    \n",
    "    # Feature scaling for test set\n",
    "    X_test_std = np.copy(X_train)\n",
    "    X_test_std[:, 0] = (X_train[:, 0] - X_train[:, 0].mean()) / X_train[:, 0].std()\n",
    "    X_test_std[:, 1] = (X_train[:, 1] - X_train[:, 1].mean()) / X_train[:, 1].std()\n",
    "    X_test_std[:, 2] = (X_train[:, 2] - X_train[:, 2].mean()) / X_train[:, 2].std()\n",
    "    \n",
    "    # instantiate and train an Adaline object\n",
    "    ada = AdalineSGD(learning_rate=learning_rate, epochs=epochs, threshold=threshold)\n",
    "    ada.fit(X_train_std, y_train)\n",
    "\n",
    "    # predict the classes of the test set and calculate accuracy\n",
    "    prediction = ada.predict(X_test_std)\n",
    "    accuracy,misclasses = accuracy_and_misclasses(prediction, y_test)\n",
    "    if verbose:\n",
    "        print(\"For features\", c1, \",\", c2, \"and\", c3, \", and test/train ratio\", testtrain_ratio, \"the perceptron had\", misclasses, \"missclassifications and had an accuracy of\", accuracy, \"\\n\")\n",
    "    return (accuracy, misclasses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Run:::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For features thickness , bland chromatin and shape uniformity , and test/train ratio 0.3 the perceptron had 82 missclassifications and had an accuracy of 0.6076555023923444 \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6076555023923444, 82)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_fit_test('thickness', 'bland chromatin', 'shape uniformity', 0.3, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're already off to a better start in terms of accuracy than with the perceptron implementation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximizing Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the classes and functions we've created so far, we can now actually utilize and test the Adaline concept on the data-set we loaded in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass 1: Maximize accuracy with respect to test/training set ratio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we try to maximize the accuracy by the way we split out training and test sets. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6149425287356322 67\n",
      "The highest accuracy was 0.6149425287356322 for test/train proportion 0.25 with 67 missclassifications.\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "misses = 0\n",
    "best_prop = 0\n",
    " \n",
    "for prop in [0.25, 0.3, 0.35, 0.40, 0.45]:\n",
    "    acc,miss = split_fit_test('thickness', 'bland chromatin', 'shape uniformity', prop)\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        misses = miss\n",
    "        best_prop = prop\n",
    "        print(acc, misses)\n",
    "        \n",
    "        \n",
    "print(\"The highest accuracy was\", best_accuracy, \"for test/train proportion\", best_prop, \"with\", misses, \"missclassifications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No significant improvement in accuracy yet, but still slightly better nonetheless! Our best test/train ratio will be 1:3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass 2: Maximize accuracy by learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we will maximize accuracy with respect to learning rate. Notice we carry over our test/train ratio which we chose the previous pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest accuracy was 0.6149425287356322 for learning rate 0.0001 with 67 missclassifications.\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "misses = 0\n",
    "best_rate = 0\n",
    "\n",
    "for rate in [0.0001,0.0005, 0.001, 0.005, 0.01]:\n",
    "    acc,miss = split_fit_test('thickness', 'bland chromatin', 'shape uniformity', 0.25, learning_rate=rate)\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        misses = miss\n",
    "        best_rate = rate\n",
    "\n",
    "print(\"The highest accuracy was\", best_accuracy, \"for learning rate\", best_rate, \"with\", misses, \"missclassifications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No improvement here, but based on the results we can assume that 0.0001 is a reasonable learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass 3: Maximize accuracy with respect to number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest accuracy was 0.6149425287356322 for 30 epochs with 67 missclassifications.\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "misses = 0\n",
    "best_num_epochs = 0\n",
    "\n",
    "for n in [10, 20, 30, 40, 50, 75, 100]:\n",
    "    acc,miss = split_fit_test('thickness', 'bland chromatin', 'shape uniformity', 0.25, epochs=n, learning_rate=0.0001)\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        misses = miss\n",
    "        best_num_epochs = n\n",
    "\n",
    "print(\"The highest accuracy was\", best_accuracy, \"for\", best_num_epochs, \"epochs with\", misses, \"missclassifications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, no change in accuracy: but we now know that we need about 30 epochs for our weights to converge to the ideal linear seperation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pass 4: Maximize accuracy with respect to threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The highest accuracy was 0.7873563218390804 for the threshold 1.0 with 37 missclassifications.\n"
     ]
    }
   ],
   "source": [
    "best_accuracy = 0\n",
    "misses = 0\n",
    "best_threshold = 0\n",
    "for theta in [0, 0.1, 0.01, 0.2, 0.5, 1.0, 2.0, 3.0]:\n",
    "    acc,miss = split_fit_test('thickness', 'bland chromatin', 'shape uniformity', 0.25, epochs=50, learning_rate=0.0005, threshold=theta)\n",
    "    if acc > best_accuracy:\n",
    "        best_accuracy = acc\n",
    "        misses = miss\n",
    "        best_threshold = theta\n",
    "        \n",
    "print(\"The highest accuracy was\", best_accuracy, \"for the threshold\", best_threshold, \"with\", misses, \"missclassifications.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems that increasing our threshold pretty drastically improved our accuracy! Compared to the perceptron, Adaline performs pretty well even without tuning our parameters. By tuning the parameters as we did in the perceptron implementation, we got a slightly higher accuracy than in the perceptron model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
